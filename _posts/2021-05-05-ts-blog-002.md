---
title: What Makes a Track Five?
categories:
- General
feature_image: "https://picsum.photos/2560/600?image=872"
---

{% include lib/mathjax.html %}

As a second part of our [Taylor Series](https://mathworld.wolfram.com/TaylorSeries.html), we want to classify whether a track by Taylor is a track five or not. With that we will use the audio features from Spotify and the created lexical ratios based on our [first blog](https://sonic-edgehog.github.io/general/2021/04/04/ts-blog-001/). For the full codes and implementation, see the corresponding notebook `ts_liners.ipynb` in the [codes](https://github.com/sonic-edgehog/sonic-edgehog.github.io/tree/main/codes) folder.

## Introduction

As we delve into one of the machine learning methods, we want to approach it as reader-friendly as possible. In layman's terms, machine learning is applying statistical techniques to identify **patterns** in a table of data. The techniques can range from [defining boundaries](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) to [quantifying the features](https://pudding.cool/2021/03/wine-model/) to (creating underlying relationships like connected neurons)[http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/]. After applying those techniques, the patterns can help  **(1) classify into a group** or **(2) predict future values**.

## Framework

For our today's post, we will explain what a decision tree is and how an ensemble of those can classify a row of features. For our example, we have two classes (track fives and non-track fives). The idea is to at least properly separate between those classes by putting boundaries. A decision tree decides on where to put those boundaries until each row can be classified into exactly one of the classes. One application of a boundary or a **split** may not be enough to perfectly sift the data hence most people would resort to the ensemble counterpart, which is **random forest**. The method applied multiple times and combinations to get the best set of boundaries for splitting the data.

For the package to be used, the following parameters are defined.

* ntrees = maximum number of trees (or unique sets containing exactly one class)
* max_depth = maximum number of splits to be done
* balance_classes = depending on the number of values per class, the binary parameter can be turned on or off.
* nfolds = number of folds for the dataset to be shuffled for different iterations.
* seed = random number used for replication purposes.

To find the best set of values for our classification problem, we would fine-tune them. Our package can do it by itself. For a more detailed discussion of the technique, see this (link)[https://datascience.stackexchange.com/questions/6015/assumptions-limitations-of-random-forest-models].

## Exploring the dataset

As we combined both datasets, we noticed that bonus tracks from `Fearless` were not included, hence we removed those. Our `track_number` column is arranged per album, we have created another binary column if a track is number five or not. We use the code block below.

```python
ts_lyric_sound_combined['track_5'] = np.where(ts_lyric_sound_combined['track_number'] == 5, 1, 0)
```

To ensure that our model will be too biased towards some features due to magnitudes, we standardized them. The code block is used below.

```python
from sklearn.preprocessing import StandardScaler
ts_lyric_sound_standard[new_features] = StandardScaler().fit_transform(ts_lyric_sound_standard[new_features])
```

To check how the distributions of the respective features could affect our model, we graphed them and calculated the respective skewness (how left-leaning or right-leaning the curve is) and kurtosis (how flat or bumpy the curve is). We excluded the `instrumentalness` feature since it has **57** values of **0**.

|                    	| Skewness 	| Kurtosis 	|
|:-------------------	|---------:	|---------:	|
| Danceability       	| -0.0987  	| 0.1548   	|
| Energy             	| -0.3359  	| -0.5892  	|
| Valence            	| 0.4139   	| -0.1888  	|
| Tempo              	| 0.2532   	| -0.6193  	|
| Loudness           	| -0.4382  	| -0.1880  	|
| Speechiness        	| 4.2878   	| 26.1806  	|
| Acousticness       	| 0.7188   	| -1.0860  	|
| Liveness           	| 1.9125   	| 3.3688   	|
| Lexical Uniqueness 	| 0.0720   	| -0.5439  	|
| Chorus Uniqueness  	| 1.2767   	| 2.2825   	|

**Table 1.** None of the features are shaped like a normal distribution curve.

{% include ts_combi_distribution.html %}

**Figure 1.** `Valence`, `tempo`, and `lexical uniqueness` have similar distributions.

Finally, we used heatmaps to graph the difference in correlation matrices between the two classes. In our dataset, we have **128** non-Track Fives and **9** Track Fives.


{% include ts_combi_heatmap_track5.html %}

**Figure 2.** The correlations between `speechiness` and **7** other features are negatively associated.

It seems that the the fifth tracks have shown better association among the features compared to the other tracks. As shown in our table below, the column with the lowest track `valence` per album is comprised of mostly track fives.


{% include taylor_table_low_sound.html %}

**Table 2.** Seven out of nine Track Five songs occupied **12** of the seventy-two positions.


{% include ts_combi_heatmap_notrack5.html %}

**Figure 3.** Almost half of the correlations are below the absolute value of **0.10**.


## track-bending the h2o

[H2o package](https://www.h2o.ai/) is an open source model building and optimization. We will use it to run a classification model and find the most important features. In particular, we will use Random Forest with the package `H2ORandomForestEstimator`. Since the model runs only with `h2oframe`, we convert it first by using `h2o.H2OFrame` and as for the label, we convert it into factor by using the `asfactor` function.

In separating the training dataset form the testing dataset, we include all tracks from the first album to `reputation` and first few tracks in `Lover`. The testing dataset will be the rest of the discography excluding `Fearless (Taylor's Version)`.

|                	| Training 	| Testing 	|
|:---------------	|---------:	|--------:	|
| Track Five     	| 7        	| 2       	|
| Non-Track Five 	| 90       	| 38      	|

**Table 3.**  The dataset is split into **71-29**.

To run the model, we have to put the predictors and the response based on our features in the `h2oframe`.

```python
predictor = new_features
response = "track_5"
ts_model = H2ORandomForestEstimator(ntrees=50, max_depth=20, nfolds=10,balance_classes = True,seed=42)
ts_model.train(x = predictor, y = response, training_frame = ts_df_train)
```

Then, a confusion matrix is created for the classification.

|                    	| Predicted Non-Track 5 	| Predicted Track 5 	|
|:-------------------	|----------------------:	|------------------:	|
| Actual Non-Track 5 	| TN = 36               	| FP = 2            	|
| Actual Track 5     	| FN = 0                	| TP = 2            	|

**Table 4.**  The model captured all track fives.

To measure how our classification works, the metric used are defined as follows:

* **Gini Coefficient** - a measurement of how well quantified the inequality is shown. The closer to 1, the more useful the classifier. In our case, the coefficient is **0.9342**.


* **Precision** - The fraction of true positives over all predicted positive labels.

$$
\begin{aligned}
precision = \frac{True Positive}{True Positive + False Positive} = \frac{2}{2+2} = \frac{1}{2}
\end{aligned}
$$

* **Recall** - The fraction of True Positive over all actual positive labels.

$$
\begin{aligned}
recall = \frac{True Positive}{True Positive + False Negative} = \frac{2}{2+0} = 1
\end{aligned}
$$

* **F1** - Assuming both measures are equally weighted, it is the harmonic mean of the `precision` and `recall`

$$
\begin{aligned}
F_1 = \frac{2*precision * recall}{precision + recall} = \frac{2*0.5*1}{0.5+1} = \frac{2}{3}
\end{aligned}
$$

* **F2** - It gives more weight to recall than precision. It is the harmonic mean of the `precision` and `recall`

$$ 
\begin{aligned}
F_2 = \frac{5*precision * recall}{4*precision + recall} = \frac{5*0.5*1}{4*0.5+1} = \frac{5}{6}
\end{aligned}
$$

* **F0.5** - It gives more weight to precision than recall. It is the harmonic mean of the `precision` and `recall`

$$ 
\begin{aligned}
F_5 = \frac{1.25*precision * recall}{0.25*precision + recall} = \frac{1.25*0.5*1}{0.25*0.5+1} = \frac{0.625}{1.125} = \frac{5}{9}
\end{aligned}
$$

Although the rates are low as what we have expected, we looked upon what features could distinguish the classes the best. By using the code block below, we plotted them in increasing order.

```python
model_var_importance = ts_model.varimp(use_pandas=True)
fig = px.bar(model_var_importance, x="percentage", y="variable", orientation='h')
fig.update_layout(title_text='Variable Importance for Classification')
fig.show()
```

{% include ts_variable_importance.html %}

**Figure 4.** Valence and Liveness contributed more than 30% to the classification.

One aspect of modelling is how we tune our parameters. In our post, we determine the optimal number of trees for our random forest to classify as correctly as possible. We have the following metrics.

* **Root Mean Squared Error** - Given a series of continuous values, it measured how close the values are compared to actual values. For our project, the predicted and actual values are 1 or 0. The score is **0.018509**, which is close to zero.

$$ 
\begin{aligned}
RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N} (y_{i} - \hat{y}_{i})^{2}}
\end{aligned}
$$

* **Log-loss** - This is a great metric for classification problems. Instead of the predicted values, the uncalibrated probabilistic values are used. The values are non-negative with zero indicating that the model has assigned the values as discriminatory as possible.

$$ 
\begin{aligned}
Log-loss = \frac{1}{N}\sum_{i=1}^{N} w_{i}(y_{i}\ln{p_{i}} + (1-y_{i})\ln(1-p_{i})
\end{aligned}
$$
