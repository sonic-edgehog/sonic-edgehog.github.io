---
title: What Makes a Track Five?
categories:
- General
feature_image: "https://picsum.photos/2560/600?image=872"
---

{% include lib/mathjax.html %}

As a second part of our [Taylor Series](https://mathworld.wolfram.com/TaylorSeries.html), we want to classify whether a track by Taylor is a track five or not. With that we will use the audio features from Spotify and the created lexical ratios based on our [first blog](https://sonic-edgehog.github.io/general/2021/04/04/ts-blog-001/). For the full codes and implementation, see the corresponding notebook `ts_liners.ipynb` in the [codes](https://github.com/sonic-edgehog/sonic-edgehog.github.io/tree/main/codes) folder.

## Exploring the dataset

As we combined both datasets, we noticed that bonus tracks from `Fearless` were not included, hence we removed those. Our `track_number` column is arranged per album, we have created another binary column if a track is number five or not. We use the code block below.

```python
ts_lyric_sound_combined['track_5'] = np.where(ts_lyric_sound_combined['track_number'] == 5, 1, 0)
```

To ensure that our model will be too biased towards some features due to magnitudes, we standardized them. The code block is used below.

```python
from sklearn.preprocessing import StandardScaler
ts_lyric_sound_standard[new_features] = StandardScaler().fit_transform(ts_lyric_sound_standard[new_features])
```

To check how the distributions of the respective features could affect our model, we graph them. We excluded the `instrumentalness` feature since it has **57** values of **0**.


{% include ts_combi_distribution.html %}

**Figure 1.** `Valence`, `tempo`, and `lexical uniqueness` have similar distributions.

Finally, we used heatmaps to graph the difference in correlation matrices between the two classes. In our dataset, we have **128** non-Track Fives and **9** Track Fives.


{% include ts_combi_heatmap_track5.html %}

**Figure 2.** The correlations between `speechiness` and **7** other features are negatively associated.

It seems that the the fifth tracks have shown better association among the features compared to the other tracks. As shown in our table below, the column with the lowest track `valence` per album is comprised of mostly track fives.


{% include taylor_table_low_sound.html %}

**Table 1.** Seven out of nine Track Five songs occupied **12** of the seventy-two positions.


{% include ts_combi_heatmap_notrack5.html %}

**Figure 2.** Almost half of the correlations are below the absolute value of **0.10**.


## Using the `h2o` package

[H2o package](https://www.h2o.ai/) is an open source model building and optimization. We will use it to run a classification model and find the most important features. In particular, we will use Random Forest with the package `H2ORandomForestEstimator`. Since the model runs only with `h2oframe`, we convert it first by using `h2o.H2OFrame` and as for the label, we convert it into factor by using the `asfactor` function.

In separating the training dataset form the testing dataset, we include all tracks from the first album to `reputation` and first few tracks in `Lover`. The testing dataset will be the rest of the discography. 

|                	| Training 	| Testing 	|
|----------------	|----------	|---------	|
| Track Five     	| 7        	| 2       	|
| Non-Track Five 	| 90       	| 38      	|

**Table 2.**  The dataset is split into **71-29**.

To run the model, we have to put the predictors and the response based on our features in the `h2oframe`.

```python
predictor = new_features
response = "track_5"
ts_model = H2ORandomForestEstimator(ntrees=50, max_depth=20, nfolds=10,balance_classes = True,seed=42)
ts_model.train(x = predictor, y = response, training_frame = ts_df_train)
```
The metric used are defined as follows:

1. `Gini Coefficient` - a measurement of how well quantified the inequality is shown. The closer to 1, the more useful the classifier. 
2. `Precision` - The fraction of true positives over all predicted positive labels.

$$
\begin{aligned}
precision = \frac{True Positive}{True Positive + False Positive} = \frac{2}{2+2} = \frac{1}{2}
\end{aligned}
$$

3. `Recall` - The fraction of True Positive over all actual positive labels.

$$
\begin{aligned}
recall = \frac{True Positive}{True Positive + False Negative} = \frac{2}{2+0} = 1
\end{aligned}
$$

4. `F1` - Assuming both measures are equally weighted, it is the harmonic mean of the `precision` and `recall`

$$
\begin{aligned}
F_1 = \frac{2*precision * recall}{precision + recall} = \frac{2*0.5*1}{0.5+1} = \frac{2}{3}
\end{aligned}
$$

5. `F2` - It gives more weight to recall than precision. It is the harmonic mean of the `precision` and `recall`

$$ 
\begin{aligned}
F_2 = \frac{5*precision * recall}{4*precision + recall} = \frac{5*0.5*1}{4*0.5+1} = \frac{5}{6}
\end{aligned}
$$

6. `F0.5` - It gives more weight to precision than recall. It is the harmonic mean of the `precision` and `recall`

$$ 
\begin{aligned}
F_5 = \frac{1.25*precision * recall}{0.25*precision + recall} = \frac{1.25*0.5*1}{0.25*0.5+1} = \frac{0.625}{1.125} = \frac{5}{9}
\end{aligned}
$$



$$
\begin{aligned}
  & \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
  = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
  & (x_1, \ldots, x_n) \left( \begin{array}{ccc}
      \phi(e_1, e_1) & \cdots & \phi(e_1, e_n) \\
      \vdots & \ddots & \vdots \\
      \phi(e_n, e_1) & \cdots & \phi(e_n, e_n)
    \end{array} \right)
  \left( \begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
    \end{array} \right)
\end{aligned}
$$
